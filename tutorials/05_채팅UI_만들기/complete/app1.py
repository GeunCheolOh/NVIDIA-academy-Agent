import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from dotenv import load_dotenv
import os

load_dotenv()

st.set_page_config(page_title="LangChain Chat", page_icon="ğŸ’¬", layout="wide")

st.title("ğŸ’¬ LangChain Chat")

MODELS = {
    "gpt-4.1-nano": "gpt-4.1-nano-2025-04-14",
    "gpt-4.1-mini": "gpt-4.1-mini-2025-04-14",
    "gpt-5-mini": "gpt-5-mini-2025-08-07",
    "gpt-5-nano": "gpt-5-nano-2025-08-07"
}

if "messages" not in st.session_state:
    st.session_state.messages = []

if "selected_model" not in st.session_state:
    st.session_state.selected_model = "gpt-4.1-mini"

if "llm" not in st.session_state:
    st.session_state.llm = ChatOpenAI(
        model=MODELS[st.session_state.selected_model],
        temperature=0.7,
        streaming=True,
        api_key=os.getenv("OPENAI_API_KEY")
    )

with st.sidebar:
    st.header("ì„¤ì •")
    
    model_choice = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        options=list(MODELS.keys()),
        index=list(MODELS.keys()).index(st.session_state.selected_model),
        key="model_selectbox"
    )
    
    if model_choice != st.session_state.selected_model:
        st.session_state.selected_model = model_choice
        st.session_state.llm = ChatOpenAI(
            model=MODELS[model_choice],
            temperature=0.7,
            streaming=True,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        st.success(f"ëª¨ë¸ì´ {model_choice}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    if st.button("ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    
    st.subheader("ëŒ€í™” íˆìŠ¤í† ë¦¬")
    if st.session_state.messages:
        total_messages = len([msg for msg in st.session_state.messages if isinstance(msg, (HumanMessage, AIMessage))])
        st.write(f"ì´ ë©”ì‹œì§€ ìˆ˜: {total_messages}")
        
        with st.expander("ì „ì²´ íˆìŠ¤í† ë¦¬ ë³´ê¸°"):
            for idx, msg in enumerate(st.session_state.messages):
                if isinstance(msg, HumanMessage):
                    st.markdown(f"**ì‚¬ìš©ì [{idx+1}]:** {msg.content}")
                elif isinstance(msg, AIMessage):
                    st.markdown(f"**AI [{idx+1}]:** {msg.content}")
    else:
        st.write("ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.markdown(message.content)

if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        for chunk in st.session_state.llm.stream(st.session_state.messages):
            full_response += chunk.content
            message_placeholder.markdown(full_response + "â–Œ")
        
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append(AIMessage(content=full_response))

