"""
rag_router_agent.py - Router ê¸°ë°˜ ë‹¤ì¤‘ ê²½ë¡œ Agent
================================================

ëª©ì :
    LLMì„ Routerë¡œ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼
    VectorDB, WebSearch, Direct LLM ì¤‘ ì ì ˆí•œ ê²½ë¡œë¥¼ ì„ íƒ

ì£¼ìš” ê¸°ëŠ¥:
    1. Router Node: LLMì´ ê²½ë¡œ ê²°ì •
    2. VectorDB Node: D2L êµì¬ ê²€ìƒ‰
    3. WebSearch Node: Tavily ì›¹ê²€ìƒ‰
    4. Direct LLM Node: LLM ì§ì ‘ ì‘ë‹µ
    5. Answer Node: ìµœì¢… ë‹µë³€ ìƒì„±
"""

import json
from typing import TypedDict, Annotated, List, Optional
import operator

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END


class AgentState(TypedDict):
    """Agentì˜ ìƒíƒœ ì •ì˜"""
    messages: Annotated[List[BaseMessage], operator.add]  # ëŒ€í™” ì´ë ¥
    question: str                    # í˜„ì¬ ì§ˆë¬¸
    route: str                       # ì„ íƒëœ ê²½ë¡œ
    routing_reason: str              # ë¼ìš°íŒ… ì´ìœ 
    search_results: str              # ê²€ìƒ‰ ê²°ê³¼
    final_answer: str                # ìµœì¢… ë‹µë³€


class RouterAgent:
    """
    Router ê¸°ë°˜ ë‹¤ì¤‘ ê²½ë¡œ RAG Agent
    
    ê²½ë¡œ:
    - vectordb: AI/ë”¥ëŸ¬ë‹ ê´€ë ¨ ì§ˆë¬¸ â†’ D2L êµì¬ ê²€ìƒ‰
    - websearch: ìµœì‹  ì •ë³´ â†’ ì›¹ê²€ìƒ‰
    - direct: ì¼ë°˜ ì§ˆë¬¸ â†’ LLM ì§ì ‘ ì‘ë‹µ
    """
    
    def __init__(
        self,
        d2l_retriever,
        api_key: str,
        model: str = "gpt-4.1-mini-2025-04-14",
        tavily_api_key: Optional[str] = None
    ):
        """
        Args:
            d2l_retriever: D2L êµì¬ ê²€ìƒ‰ê¸°
            api_key: OpenAI API í‚¤
            model: ì‚¬ìš©í•  LLM ëª¨ë¸
            tavily_api_key: Tavily API í‚¤
        """
        self.d2l_retriever = d2l_retriever
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model=model,
            temperature=0.3,
            api_key=api_key
        )
        
        # Tavily ì›¹ê²€ìƒ‰ ë„êµ¬
        if tavily_api_key:
            self.tavily_tool = TavilySearchResults(
                max_results=3,
                api_key=tavily_api_key
            )
        else:
            self.tavily_tool = None
        
        # Agent ê·¸ë˜í”„ ìƒì„±
        self.agent = self._build_graph()
    
    def _build_graph(self):
        """LangGraph ìƒíƒœ ê·¸ë˜í”„ë¥¼ êµ¬ì„±"""
        workflow = StateGraph(AgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("router", self._router_node)
        workflow.add_node("vectordb", self._vectordb_node)
        workflow.add_node("websearch", self._websearch_node)
        workflow.add_node("direct_llm", self._direct_llm_node)
        workflow.add_node("answer", self._answer_node)
        
        # ì‹œì‘ì 
        workflow.set_entry_point("router")
        
        # ì¡°ê±´ë¶€ ì—£ì§€: routerì—ì„œ ê²½ë¡œ ë¶„ê¸°
        workflow.add_conditional_edges(
            "router",
            self._route_question,
            {
                "vectordb": "vectordb",
                "websearch": "websearch",
                "direct": "direct_llm"
            }
        )
        
        # vectordb/websearch â†’ answer
        workflow.add_edge("vectordb", "answer")
        workflow.add_edge("websearch", "answer")
        
        # direct_llm â†’ END (ë‹µë³€ ì´ë¯¸ ìƒì„±ë¨)
        workflow.add_edge("direct_llm", END)
        
        # answer â†’ END
        workflow.add_edge("answer", END)
        
        return workflow.compile()
    
    def _router_node(self, state: AgentState) -> dict:
        """
        Router ë…¸ë“œ: LLMì´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ê²½ë¡œ ê²°ì •
        
        Args:
            state: í˜„ì¬ Agent ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸í•  ìƒíƒœ (route, routing_reason)
        """
        question = state["question"]
        
        router_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì²˜ë¦¬ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ì„ íƒì§€:
1. **vectordb**: AI, ë”¥ëŸ¬ë‹, ë¨¸ì‹ ëŸ¬ë‹, ì‹ ê²½ë§, ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë“± AI/ML ê¸°ìˆ ì  ì§ˆë¬¸
   - ì˜ˆ: "backpropagationì´ë€?", "CNNì˜ êµ¬ì¡°ëŠ”?", "gradient descent ì„¤ëª…"
   - ì¶œì²˜: D2L (Dive into Deep Learning) êµì¬

2. **websearch**: ìµœì‹  ë‰´ìŠ¤, ì‹¤ì‹œê°„ ì •ë³´, 2023ë…„ ì´í›„ ì´ë²¤íŠ¸, í˜„ì¬ ë‚ ì”¨/ì£¼ê°€ ë“±
   - ì˜ˆ: "2024ë…„ ë…¸ë²¨ìƒ", "ì˜¤ëŠ˜ ë‚ ì”¨", "ìµœì‹  AI ë‰´ìŠ¤"
   - ì¶œì²˜: ì›¹ ê²€ìƒ‰

3. **direct**: ì¼ë°˜ ëŒ€í™”, ë²ˆì—­, ê³„ì‚°, ì¶”ë¡ , ì°½ì‘ ë“±
   - ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”", "1+1ì€?", "ì‹œ ì¨ì¤˜", "Python ì½”ë“œ ì‘ì„±"
   - ì¶œì²˜: LLM ì§ì ‘ ì‘ë‹µ

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ì‘ë‹µ:
{{
    "route": "vectordb" ë˜ëŠ” "websearch" ë˜ëŠ” "direct",
    "reasoning": "ì„ íƒí•œ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = self.llm.invoke([SystemMessage(content=router_prompt)])
            result = json.loads(response.content)
            
            route = result.get("route", "direct")
            reasoning = result.get("reasoning", "ê¸°ë³¸ ê²½ë¡œ ì„ íƒ")
            
            # ìœ íš¨ì„± ê²€ì‚¬
            if route not in ["vectordb", "websearch", "direct"]:
                route = "direct"
                reasoning = "ì•Œ ìˆ˜ ì—†ëŠ” ê²½ë¡œ, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©"
            
            print(f"ğŸ§­ Router ê²°ì •: {route}")
            print(f"   ì´ìœ : {reasoning}")
            
            return {
                "route": route,
                "routing_reason": reasoning
            }
            
        except Exception as e:
            print(f"âš ï¸ Router ì˜¤ë¥˜: {str(e)}, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©")
            return {
                "route": "direct",
                "routing_reason": f"ë¼ìš°íŒ… ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    def _vectordb_node(self, state: AgentState) -> dict:
        """
        VectorDB ë…¸ë“œ: D2L êµì¬ì—ì„œ ê²€ìƒ‰
        
        Args:
            state: í˜„ì¬ Agent ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸í•  ìƒíƒœ (search_results)
        """
        question = state["question"]
        
        try:
            print(f"ğŸ“š D2L êµì¬ ê²€ìƒ‰: '{question}'")
            docs = self.d2l_retriever.invoke(question)
            
            if docs:
                results = "\n\n".join([
                    f"[ë¬¸ì„œ {i+1}]\n{doc.page_content}" 
                    for i, doc in enumerate(docs)
                ])
                print(f"âœ… {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            else:
                results = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            return {"search_results": results}
            
        except Exception as e:
            print(f"âŒ VectorDB ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return {"search_results": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
    
    def _websearch_node(self, state: AgentState) -> dict:
        """
        WebSearch ë…¸ë“œ: Tavilyë¡œ ì›¹ ê²€ìƒ‰
        
        Args:
            state: í˜„ì¬ Agent ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸í•  ìƒíƒœ (search_results)
        """
        question = state["question"]
        
        if not self.tavily_tool:
            return {
                "search_results": "ì›¹ ê²€ìƒ‰ ë„êµ¬ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Tavily API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            }
        
        try:
            print(f"ğŸŒ ì›¹ ê²€ìƒ‰: '{question}'")
            search_results = self.tavily_tool.invoke(question)
            
            if search_results:
                results = "\n\n".join([
                    f"[{r.get('title', 'ì œëª© ì—†ìŒ')}]\n{r.get('content', '')}" 
                    for r in search_results
                ])
                print(f"âœ… {len(search_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì™„ë£Œ")
            else:
                results = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            return {"search_results": results}
            
        except Exception as e:
            print(f"âŒ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return {"search_results": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
    
    def _direct_llm_node(self, state: AgentState) -> dict:
        """
        Direct LLM ë…¸ë“œ: LLMì— ì§ì ‘ ì§ˆë¬¸
        
        Args:
            state: í˜„ì¬ Agent ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸í•  ìƒíƒœ (final_answer)
        """
        question = state["question"]
        messages = state.get("messages", [])
        
        try:
            print(f"ğŸ’¬ LLM ì§ì ‘ ì‘ë‹µ: '{question}'")
            
            # ëŒ€í™” ì´ë ¥ í¬í•¨
            conversation = messages + [HumanMessage(content=question)]
            response = self.llm.invoke(conversation)
            
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            return {
                "final_answer": response.content,
                "messages": [
                    HumanMessage(content=question),
                    AIMessage(content=response.content)
                ]
            }
            
        except Exception as e:
            print(f"âŒ LLM ì‘ë‹µ ì‹¤íŒ¨: {str(e)}")
            return {
                "final_answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "messages": []
            }
    
    def _answer_node(self, state: AgentState) -> dict:
        """
        Answer ë…¸ë“œ: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        
        Args:
            state: í˜„ì¬ Agent ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸í•  ìƒíƒœ (final_answer, messages)
        """
        question = state["question"]
        search_results = state["search_results"]
        route = state["route"]
        messages = state.get("messages", [])
        
        # direct ê²½ë¡œëŠ” ì´ë¯¸ ë‹µë³€ì´ ìƒì„±ë˜ì–´ ìˆìŒ
        if route == "direct":
            return {}
        
        try:
            print(f"âœï¸  ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
            
            # ëŒ€í™” ì´ë ¥ í¬ë§·íŒ…
            history = ""
            if messages:
                history = "\nì´ì „ ëŒ€í™”:\n"
                for msg in messages[-4:]:  # ìµœê·¼ 4ê°œë§Œ
                    role = "ì‚¬ìš©ì" if isinstance(msg, HumanMessage) else "AI"
                    history += f"{role}: {msg.content[:100]}...\n"
            
            answer_prompt = f"""{history}

ì§ˆë¬¸: {question}

ì°¸ê³  ìë£Œ (ì¶œì²˜: {'D2L êµì¬' if route == 'vectordb' else 'ì›¹ ê²€ìƒ‰'}):
{search_results}

ìœ„ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì°¸ê³  ìë£Œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”."""

            response = self.llm.invoke([SystemMessage(content=answer_prompt)])
            
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            return {
                "final_answer": response.content,
                "messages": [
                    HumanMessage(content=question),
                    AIMessage(content=response.content)
                ]
            }
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "final_answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "messages": []
            }
    
    def _route_question(self, state: AgentState) -> str:
        """
        ë¼ìš°íŒ… í•¨ìˆ˜: state["route"]ë¥¼ ë³´ê³  ë‹¤ìŒ ë…¸ë“œ ê²°ì •
        
        Args:
            state: í˜„ì¬ Agent ìƒíƒœ
            
        Returns:
            ë‹¤ìŒ ë…¸ë“œ ì´ë¦„
        """
        return state["route"]
    
    def invoke(self, question: str, chat_history: Optional[List[BaseMessage]] = None) -> dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            chat_history: ì´ì „ ëŒ€í™” ì´ë ¥
            
        Returns:
            ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            {
                "question": ì§ˆë¬¸,
                "route": ì„ íƒëœ ê²½ë¡œ,
                "routing_reason": ë¼ìš°íŒ… ì´ìœ ,
                "search_results": ê²€ìƒ‰ ê²°ê³¼ (ìˆëŠ” ê²½ìš°),
                "answer": ìµœì¢… ë‹µë³€
            }
        """
        print("\n" + "=" * 60)
        print(f"ì§ˆë¬¸: {question}")
        print("=" * 60)
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = {
            "messages": chat_history or [],
            "question": question,
            "route": "",
            "routing_reason": "",
            "search_results": "",
            "final_answer": ""
        }
        
        # Agent ì‹¤í–‰
        result = self.agent.invoke(initial_state)
        
        return {
            "question": question,
            "route": result.get("route", "unknown"),
            "routing_reason": result.get("routing_reason", ""),
            "search_results": result.get("search_results", ""),
            "answer": result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        }

