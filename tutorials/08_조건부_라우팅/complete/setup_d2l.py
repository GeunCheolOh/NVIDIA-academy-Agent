"""
setup_d2l.py - D2L êµì¬ ë‹¤ìš´ë¡œë“œ ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
======================================================

ëª©ì :
    D2L (Dive into Deep Learning) êµì¬ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ 
    ë²¡í„° ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

ì‚¬ìš©:
    python setup_d2l.py
"""

import os
import requests
from pathlib import Path

from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv

load_dotenv()

# ì„¤ì •
PDF_URL = "https://d2l.ai/d2l-en.pdf"
PDF_PATH = "d2l-en.pdf"
CHROMA_DB_PATH = "./chroma_db_d2l"
MAX_PAGES = 100  # ì²˜ìŒ 100í˜ì´ì§€ë§Œ ì²˜ë¦¬ (ì „ì²´ëŠ” ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼)


def download_pdf(url: str, path: str) -> bool:
    """
    PDF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        url: PDF URL
        path: ì €ì¥ ê²½ë¡œ
        
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    if Path(path).exists():
        print(f"âœ… PDF íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {path}")
        return True
    
    try:
        print(f"ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {url}")
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
                downloaded += len(chunk)
                # ì§„í–‰ë¥  í‘œì‹œ
                if total_size > 0:
                    progress = (downloaded / total_size) * 100
                    print(f"\rì§„í–‰: {progress:.1f}%", end='')
        
        print(f"\nâœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {path}")
        return True
        
    except Exception as e:
        print(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False


def setup_vectorstore(
    pdf_path: str,
    chroma_path: str,
    max_pages: int = None
) -> Chroma:
    """
    PDFë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        chroma_path: Chroma DB ì €ì¥ ê²½ë¡œ
        max_pages: ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        
    Returns:
        Chroma ë²¡í„° ìŠ¤í† ì–´ ê°ì²´
    """
    # ì´ë¯¸ ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ë¡œë“œ
    if Path(chroma_path).exists():
        print(f"âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {chroma_path}")
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=os.getenv("OPENAI_API_KEY")
        )
        vectorstore = Chroma(
            persist_directory=chroma_path,
            embedding_function=embeddings
        )
        count = vectorstore._collection.count()
        print(f"âœ… {count}ê°œì˜ ë²¡í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return vectorstore
    
    print(f"ğŸ”¨ ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    
    # 1. PDF ë¡œë”©
    print(f"ğŸ“– PDF ë¡œë”© ì¤‘: {pdf_path}")
    loader = PyMuPDFLoader(pdf_path)
    documents = loader.load()
    
    if max_pages:
        documents = documents[:max_pages]
        print(f"âœ… {len(documents)}ê°œ í˜ì´ì§€ ë¡œë“œ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
    else:
        print(f"âœ… {len(documents)}ê°œ í˜ì´ì§€ ë¡œë“œ")
    
    # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
    print("âœ‚ï¸  í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_documents(documents)
    print(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±")
    
    # 3. ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    print("ğŸ”¢ ì„ë² ë”© ìƒì„± ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘...")
    print("   (ì´ ì‘ì—…ì€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=chroma_path
    )
    
    count = vectorstore._collection.count()
    print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {count}ê°œ ë²¡í„°")
    
    return vectorstore


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("D2L êµì¬ ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •")
    print("=" * 60)
    
    # 1. PDF ë‹¤ìš´ë¡œë“œ
    if not download_pdf(PDF_URL, PDF_PATH):
        print("âŒ ì„¤ì • ì‹¤íŒ¨")
        return
    
    # 2. ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
    try:
        vectorstore = setup_vectorstore(
            PDF_PATH,
            CHROMA_DB_PATH,
            MAX_PAGES
        )
        
        # 3. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        print("\n" + "=" * 60)
        print("í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
        print("=" * 60)
        
        test_query = "What is deep learning?"
        print(f"ì§ˆë¬¸: {test_query}")
        
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        docs = retriever.invoke(test_query)
        
        print(f"\nê²€ìƒ‰ ê²°ê³¼ ({len(docs)}ê°œ ë¬¸ì„œ):")
        for i, doc in enumerate(docs, 1):
            print(f"\n[ë¬¸ì„œ {i}]")
            print(doc.page_content[:200] + "...")
        
        print("\n" + "=" * 60)
        print("âœ… ì„¤ì • ì™„ë£Œ!")
        print("=" * 60)
        print(f"ë²¡í„° ìŠ¤í† ì–´ ìœ„ì¹˜: {CHROMA_DB_PATH}")
        print("ì´ì œ app_router.pyë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

