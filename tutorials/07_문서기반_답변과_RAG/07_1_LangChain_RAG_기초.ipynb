{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc4f3bb",
   "metadata": {},
   "source": [
    "# Langchain RAG 시스템\n",
    "\n",
    "이 노트북은 data 폴더의 PDF 파일들을 활용하여 RAG (Retrieval-Augmented Generation) 시스템을 구현합니다.\n",
    "\n",
    "## 주요 기능\n",
    "1. PDF 문서 로딩 및 전처리\n",
    "2. 텍스트 청킹 (chunking)\n",
    "3. 임베딩 생성 및 벡터 스토어 구축\n",
    "4. 검색 기반 질문-답변 시스템\n",
    "\n",
    "## 사용할 데이터(데이터 출처: 위키피디아)\n",
    "- 귀신고래.pdf\n",
    "- 범고래.pdf  \n",
    "- 흰꼬리수리.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f04a9d",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai langchain-community chromadb pymupdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import userdata   \n",
    "    api_key = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"Colab Secrets에서 API 키를 성공적으로 불러왔습니다.\")\n",
    "except:\n",
    "    import getpass\n",
    "    api_key = getpass.getpass(\"OpenAI API 키를 입력하세요: \")\n",
    "    print(\"API 키가 입력되었습니다.\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed060dcc",
   "metadata": {},
   "source": [
    "## 인덱싱(Indexing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a357bb",
   "metadata": {},
   "source": [
    "### 문서 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 폴더에서 PDF 파일 목록 가져오기\n",
    "data_path = Path(\"data\")\n",
    "pdf_files = list(data_path.glob(\"*.pdf\"))\n",
    "\n",
    "# 모든 PDF 문서를 담을 리스트\n",
    "all_documents = []\n",
    "\n",
    "# 각 PDF 파일을 순회하며 로드\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"불러오는 중 : {pdf_file}\")\n",
    "    loader = PyMuPDFLoader(str(pdf_file))\n",
    "    documents = loader.load()\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "print(\"파일 유형\", type(all_documents[0]))\n",
    "print(\"파일 수\", len(all_documents))\n",
    "print(f\"페이지 별 글자 수: {[len(x.page_content) for x in all_documents]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a26b9",
   "metadata": {},
   "source": [
    "### 청크 분할(Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(all_documents)\n",
    "\n",
    "print(\"철자 단위 청킹 후 문서 수\", len(chunks))\n",
    "print(\"청크 별 글자 수\", [len(x.page_content) for x in chunks])\n",
    "print(\"-\"*50)\n",
    "print(chunks[0].page_content[600:])\n",
    "print(\"-\"*50)\n",
    "print(chunks[1].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd242719",
   "metadata": {},
   "source": [
    "### 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Chroma 벡터 스토어 생성 및 저장\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(f\"벡터 스토어에 {vectorstore._collection.count()}개의 벡터 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_id = vectorstore.get()['ids'][0]  # 첫 번째 청크의 id 자동 추출\n",
    "info = vectorstore.get(ids=[first_id], include=[\"embeddings\", \"documents\", \"metadatas\"])\n",
    "\n",
    "print(f\"첫 번째 청크의 인덱스: {info['ids']}\")\n",
    "print(f\"청크 임베딩 차원: {info['embeddings'][0].shape}\\n값: {info['embeddings'][0][:10]}...\")\n",
    "print(f\"청크 문서: {info['documents'][0]}\")\n",
    "print(f\"청크 메타데이터: {info['metadatas'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05bb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.search(query=\"범고래의 먹이\",\n",
    "                   search_type=\"similarity_score_threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268706d",
   "metadata": {},
   "source": [
    "### 검색기 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색기(Retriever) 생성\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "retriever.invoke(\"범고래의 먹이\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91159d",
   "metadata": {},
   "source": [
    "## RAG 체인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe778f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 프롬프트\n",
    "messages = [\n",
    "    (\"system\", \"다음 문맥을 사용하여 질문에 답하세요. 문맥에서 답을 찾을 수 없다면, 모른다고 말하세요.\"),\n",
    "    (\"human\", \"문맥: {context}\\n질문: {question}.\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "# 검색된 문서 리스트를 하나의 문자열로 합치는 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# 체인 생성\n",
    "rag_chain = (\n",
    "    # context와 question을 딕셔너리 형태로 구성\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"범고래는 어떤 먹이를 먹나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d36300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또는 아래와 같은 방식으로도 체인 구성 가능\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 프롬프트\n",
    "template = \"\"\"다음 문맥을 사용하여 질문에 답하세요. 문맥에서 답을 찾을 수 없다면, 모른다고 말하세요.\n",
    "\n",
    "문맥:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변: 위의 문맥을 바탕으로 질문에 대해 자세하고 정확한 답변을 한국어로 제공해주세요.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# RetrievalQA 체인 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_wiki(question):\n",
    "    print(f\"질문: {question}\")\n",
    "\n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    answer = response[\"result\"]\n",
    "    print(f\"답변: {answer}\")\n",
    "    \n",
    "    source_documents = response[\"source_documents\"]\n",
    "    print(f\"참고 문서: {source_documents}\")\n",
    "\n",
    "answer_with_wiki(\"범고래는 어떤 먹이를 먹나요?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
