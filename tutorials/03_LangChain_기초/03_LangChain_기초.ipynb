{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e78de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core langchain-openai langchain-community pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7662d",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 모델 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1908f7c",
   "metadata": {},
   "source": [
    "### 기본 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c1f67",
   "metadata": {},
   "source": [
    "###  OpenAI 키 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa314314",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"Colab Secrets에서 API 키를 성공적으로 불러왔습니다.\")\n",
    "except (ImportError, KeyError):\n",
    "    try:\n",
    "        api_key = getpass.getpass(\"OpenAI API 키를 입력하세요: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        print(\"API 키가 입력되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"API 키를 설정하는 중 오류가 발생했습니다: {e}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e637ce",
   "metadata": {},
   "source": [
    "## 2. LLM 및 기본 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c2ad4",
   "metadata": {},
   "source": [
    "### OpenAI LLM 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af490a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "response1 = llm_openai.invoke(\"태양계에서 가장 큰 행성은?\")\n",
    "print(response1.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af13d6e",
   "metadata": {},
   "source": [
    "### 파라미터를 적용한 OpenAI LLM 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f646ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai_params = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=150,\n",
    ")\n",
    "\n",
    "response3 = llm_openai_params.invoke(\"태양계에서 두 번째로 큰 행성은?\")\n",
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fcf9f",
   "metadata": {},
   "source": [
    "### 프롬프트 템플릿 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ed3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# 메시지 리스트를 사용한 호출\n",
    "messages = [\n",
    "    SystemMessage(content=\"당신은 우주에 대해 모든 것을 알고 있는 천문학자입니다.\"),\n",
    "    HumanMessage(content=\"우리 은하에서 가장 가까운 은하는?\"),\n",
    "]\n",
    "response4 = llm_openai.invoke(messages)\n",
    "print(response4.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# MessagesPlaceholder는 e. 메모리 파트에서 다룰 것이므로 여기서는 제외합니다.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 {country} 요리 전문 셰프입니다. 해당 분야에 대해서만 자세하게 답변하고, 다른 나라 음식에 대해 질문할 경우 {country} 방식으로 재해석한 레시피를 제안하세요.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "formatted_prompt = prompt_template.invoke({\n",
    "    \"country\": \"한국의 전통 남도식\",\n",
    "    \"question\": \"까르보나라를 만드는 가장 전통적인 방법은 무엇인가요?\",\n",
    "})\n",
    "print(formatted_prompt.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                        temperature=0.0,\n",
    "                        max_tokens=1000,\n",
    "                        )\n",
    "\n",
    "llm_openai.invoke(formatted_prompt) # 또는 llm_openai.invoke(formatted_prompt.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d756c49",
   "metadata": {},
   "source": [
    "### String output parser 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StrOutputParser의 기능\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm_output = llm_openai.invoke(formatted_prompt.to_messages())\n",
    "parsed_output = output_parser.invoke(llm_output)\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0532ae",
   "metadata": {},
   "source": [
    "## 3. LCEL(LangXhain Expression Language)와 체인\n",
    "\n",
    "- LangChain의 구성요소(Prompt, LLM, Parser 등)는 'Runnable' 프로토콜을 따릅니다.\n",
    "- 이는 각 요소가 `.invoke()`, `.batch()`, `.stream()` 메소드를 가지고 있음을 의미하며,\n",
    "- 이 덕분에 파이프(`|`) 연산자로 쉽게 연결하여 '체인'을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL을 이용한 간단한 체인 구성\n",
    "\n",
    "chain = prompt_template | llm_openai | output_parser\n",
    "\n",
    "# .invoke(): 하나의 입력을 받아 결과를 반환합니다.\n",
    "response_from_chain = chain.invoke({\n",
    "    \"country\": \"한국의 전통 남도식\",\n",
    "    \"question\": \"시카고 피자 만드는 방법 알려주세요.\",\n",
    "})\n",
    "print(response_from_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .batch(): 여러 입력을 리스트로 받아 결과 리스트를 반환합니다. (병렬 처리로 더 빠름)\n",
    "batch_inputs = [\n",
    "    {\"country\": \"일본\", \"question\": \"스시의 역사에 대해 알려줘.\"},\n",
    "    {\"country\": \"프랑스\", \"question\": \"크루아상의 기원은 어디야?\"},\n",
    "]\n",
    "batch_responses = chain.batch(batch_inputs)\n",
    "for res in batch_responses:\n",
    "    print(f\"- {res[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f11d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .stream(): 결과가 생성되는 대로 실시간으로 조각(chunk)을 받아 처리합니다.\n",
    "stream = chain.stream({\n",
    "    \"country\": \"인도\",\n",
    "    \"question\": \"치킨 티카 마살라 레시피 알려줘.\",\n",
    "})\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print() # 마지막에 줄바꿈을 위해 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5635ff",
   "metadata": {},
   "source": [
    "## 4. 메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# 메모리 테스트를 위한 간단한 체인 구성\n",
    "prompt_for_memory = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 사용자와 대화하는 친절한 AI입니다.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chat_chain = prompt_for_memory | llm_openai | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history를 직접 빈 리스트로 전달할 경우 대화가 이어지지 않음\n",
    "print(\"User: 제 이름은 오근철입니다.\")\n",
    "response = chat_chain.invoke({\"input\": \"제 이름은 오근철입니다.\", \"history\": []})\n",
    "print(f\"AI: {response}\")\n",
    "\n",
    "print(\"\\nUser: 제 이름이 뭐라고 했죠?\")\n",
    "response = chat_chain.invoke({\"input\": \"제 이름이 뭐라고 했죠?\", \"history\": []})\n",
    "print(f\"AI: {response}\") # 이름을 기억하지 못합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리가 추가된 체인\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    lambda session_id: memory.chat_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "config = {\"configurable\": {\"session_id\": \"user_session_1\"}}\n",
    "\n",
    "print(\"User: 제 이름은 오근철입니다.\")\n",
    "response = chain_with_memory.invoke({\"input\": \"제 이름은 오근철입니다.\"}, config=config)\n",
    "print(f\"AI: {response}\")\n",
    "\n",
    "print(\"\\nUser: 제 이름이 뭐라고 했죠?\")\n",
    "response = chain_with_memory.invoke({\"input\": \"제 이름이 뭐라고 했죠?\"}, config=config)\n",
    "print(f\"AI: {response}\") # 이름을 정확히 기억하고 대답합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Assistant 메시지를 이용한 수동 메모리 관리\n",
    "\n",
    "# 대화 기록을 수동으로 관리할 리스트\n",
    "manual_history = []\n",
    "\n",
    "# 첫 번째 질문\n",
    "print(\"\\nUser: 제 취미는 독서입니다. 영국 소설을 좋아해요.\")\n",
    "user_input_1 = \"제 취미는 독서입니다. 영국 소설을 좋아해요.\"\n",
    "manual_history.append(HumanMessage(content=user_input_1))\n",
    "response_manual_1 = llm_openai.invoke(prompt_for_memory.invoke({\"input\": user_input_1, \"history\": manual_history}))\n",
    "\n",
    "# AI의 답변(AIMessage)을 기록에 추가\n",
    "manual_history.append(response_manual_1)\n",
    "print(f\"AI: {response_manual_1.content}\")\n",
    "\n",
    "# 두 번째 질문\n",
    "print(\"\\nUser: 특히, 그 중에서 추리소설을 좋아해요.\")\n",
    "user_input_2 = \"특히, 그 중에서 추리소설을 좋아해요.\"\n",
    "manual_history.append(HumanMessage(content=user_input_2))\n",
    "\n",
    "# 이전 대화 기록이 담긴 history를 함께 전달\n",
    "response_manual_2 = llm_openai.invoke(prompt_for_memory.invoke({\"input\": user_input_2, \"history\": manual_history}))\n",
    "print(f\"AI: {response_manual_2.content}\") # 취미를 정확히 기억하고 대답합니다.\n",
    "\n",
    "print(\"\\nUser: 제 취향을 고려해서 재미있는 책들을 추천해주세요.\")\n",
    "user_input_3 = \"제 취향을 고려해서 재미있는 책들을 추천해주세요.\"\n",
    "manual_history.append(HumanMessage(content=user_input_3))\n",
    "\n",
    "response_manual_3 = llm_openai.invoke(prompt_for_memory.invoke({\"input\": user_input_3, \"history\": manual_history}))\n",
    "print(f\"AI: {response_manual_3.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b4c9b",
   "metadata": {},
   "source": [
    "## 5. 출력 구조화를 위한 파서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47960a7e",
   "metadata": {},
   "source": [
    "### JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab92fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JsonOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "json_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"문장에서 '인물', '장소', '시간' 정보를 추출하여 JSON으로 출력해줘.\n",
    "    문장: {sentence}\n",
    "    JSON 출력:\"\"\"\n",
    ")\n",
    "json_chain = json_prompt | llm_openai | JsonOutputParser()\n",
    "json_output = json_chain.invoke({\"sentence\": \"내일 오후 3시에 서울역에서 김철수 씨를 만나기로 했어요.\"})\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b1792",
   "metadata": {},
   "source": [
    "### Pydantic Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf04769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PydanticOutputParser\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# 파서 클래스 정의\n",
    "class MeetingDetails(BaseModel):\n",
    "    location: Optional[str] = Field(description=\"회의가 열리는 장소\")\n",
    "    time: Optional[str] = Field(description=\"회의 시작 시간\")\n",
    "    attendees: List[str] = Field(description=\"회의 참석자들의 이름 목록\", default=[])\n",
    "\n",
    "# 파서 생성\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=MeetingDetails)\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "pydantic_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"주어진 이메일 본문에서 회의 정보를 추출해줘.\n",
    "    {format_instructions}\n",
    "    이메일 본문: --- {email_body} ---\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# 파서가 적용된 체인 생성\n",
    "pydantic_chain = pydantic_prompt | llm_openai | pydantic_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_email = \"\"\"\n",
    "안녕하세요,\n",
    "다음 주 프로젝트 리뷰 회의 일정을 공유드립니다.\n",
    "- 일시: 2025년 8월 20일 (수) 오후 2시\n",
    "- 장소: 본사 3층 대회의실\n",
    "- 참석자: 김태균 팀장, 안가영 선임, 조예찬 책임\n",
    "감사합니다.\n",
    "\"\"\"\n",
    "meeting_info = pydantic_chain.invoke({\n",
    "    \"email_body\": sample_email,\n",
    "    \"format_instructions\": pydantic_parser.get_format_instructions()\n",
    "})\n",
    "print(meeting_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
